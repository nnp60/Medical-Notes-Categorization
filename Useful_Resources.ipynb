{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Useful Resources\n",
        "\n",
        "**LLMs for Electronic Health Records**:\n",
        "- https://www.nature.com/articles/s41746-022-00742-2?fromPaywallRec=false\n",
        "\n",
        "**John Snow Labs Integration**:\n",
        "- https://youtu.be/VhxJd1L7KiA?feature=shared\n",
        "\n",
        "**SmarterDX Innovation**:\n",
        "- https://www.linkedin.com/posts/smarterdx_artificial-intelligence-in-healthcare-defining-activity-7188368404666290176-2Xaa/\n",
        "\n",
        "**BERT Transformer**:\n",
        "- https://medium.com/@anmolkohli/my-notes-on-bert-tokenizer-and-model-98dc22d0b64#:~:text=What%20is%20BERT%20Tokenizer%3F,boy'%20and%20's'.\n",
        "\n",
        "**Other Notes**:\n",
        "\n",
        "NLP sits at the middle of unstructured and unstructured data.\n",
        "- NL Understanding is going from unstructured to structured.\n",
        "- NL Generation us going from structured to unstructured.\n",
        "- NLP is not one algorithm. It is a bag of tools used to analyze use cases like machine translation, chatbots/virtual assistants, sentiment analysis or product reviews, etc.\n",
        "\n",
        "Steps\n",
        "1. Inputs are usually unstructured text\n",
        "2. That string text is tokenized or broken down into smaller, various, meaningful chunks. Each chunk is a token and we work with one token at a time.\n",
        "3. After tokenizing, we perform Stemming or deriving the word stem for a given token. For ex: running, ran, runs all have the word stem \"run\".\n",
        "4. Some instances don't work such as university and universal so we employ a technique called lemmatization. Lemmatization takes a given token and learns its meaning through a dictionary definition. From there it derives the word's root or lemm. For ex: the word \"better\" is derived from the word \"good\". Good would be the root/lemm here but the stem would be \"bet\" which is not an accurate depiction.\n",
        "5. Once we stem or lemm, we identify the context of that token in the overall sentence through a process called Part of Speech Tagging. This is identifying the use of words. For ex: \"Can I make you a cake?\" and \"What make is your laptop?\" have two vastly different uses of the word \"make\".\n",
        "6. Lastly, we do NER: Name Entity Recognition or finding entities or meanings associated with tokens. Arizona is a US state while Ralph is a person's name.\n",
        "\n",
        "Transformers are forms of deep learning architecture that process input data and capture context and relevance in a suitable output. Encoders take input like string text and convert it into a format appropriate for storage or transmission. Decoders perform the backwards operations.\n",
        "- BERT is an encoder transformation that performs MLM or masked langauge modeling and NSP or next sentence prediction. DistillBert is the production form of BERT that uses less memory and is more efficient. RoBERTa is an optimized BERT model that has an adjusted pre-training objective. It drops the NSP function and is trained on longer sentences leading to improved performance. AlBERT trains for larger models with fewer parameters."
      ],
      "metadata": {
        "id": "HP1CpJ3Y-8nn"
      }
    }
  ]
}